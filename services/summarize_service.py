import re
import logging
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from utils.news_cleaner import clean_news
from typing import Dict, List, Tuple

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
model = None
tokenizer = None


def load_model():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤"""
    global model, tokenizer
    logger.info("ğŸ”„ T5 ëª¨ë¸ ë¡œë”© ì‹œì‘...")
    model = AutoModelForSeq2SeqLM.from_pretrained("paust/pko-t5-base")
    tokenizer = AutoTokenizer.from_pretrained("paust/pko-t5-base")
    logger.info("âœ… T5 ëª¨ë¸ ë¡œë”© ì™„ë£Œ")


def detect_report_type(text: str) -> Dict[str, any]:
    """ì œë³´ì˜ ìœ í˜•ê³¼ íŠ¹ì§•ì„ ì •ë°€í•˜ê²Œ ê°ì§€í•©ë‹ˆë‹¤."""
    logger.info("ğŸ” ì œë³´ ìœ í˜• ê°ì§€ ì‹œì‘...")

    accident_types = {
        'êµí†µì‚¬ê³ ': ['êµí†µì‚¬ê³ ', 'ì°¨ëŸ‰ì‚¬ê³ ', 'ì¶©ëŒ', 'ì¶”ëŒ', 'ì ‘ì´‰ì‚¬ê³ ', 'ì°¨ëŸ‰', 'ìš´ì „', 'ì‚¬ê³ ì°¨', 'ì‹ í˜¸ìœ„ë°˜'],
        'í™”ì¬': ['í™”ì¬', 'ë¶ˆ', 'ì—°ê¸°', 'ì†Œë°©ì°¨', 'ì „ì†Œ', 'í­ë°œ', 'ë¶ˆê¸¸', 'ë¶ˆê½ƒ'],
        'ë²”ì£„': ['ì ˆë„', 'ê°•ë„', 'í­í–‰', 'ì‚¬ê¸°', 'ì„±ë²”ì£„', 'ì„±ì¶”í–‰', 'ë„ë‘‘', 'ì‚´ì¸', 'ì´ê²©'],
        'ìì—°ì¬í•´': ['íƒœí’', 'ì§€ì§„', 'í™ìˆ˜', 'ì¹¨ìˆ˜', 'ì‚°ì‚¬íƒœ', 'í­ìš°', 'í­ì„¤', 'ê°•í’', 'ê¸°ìƒì´ë³€'],
        'ê±´ì„¤ì‚¬ê³ ': ['ê±´ì„¤í˜„ì¥', 'ê³µì‚¬í˜„ì¥', 'ë¶•ê´´', 'ì¶”ë½', 'í¬ë ˆì¸', 'ê±´ë¬¼ë¶•ê´´', 'ì½˜í¬ë¦¬íŠ¸'],
        'ì˜ë£Œì‚¬ê³ ': ['ë³‘ì›', 'ì‘ê¸‰', 'ì˜ë£Œì‚¬ê³ ', 'ì§„ë£Œ', 'ì˜ë£Œì§„', 'ê³¼ì‹¤', 'ìˆ˜ìˆ  ì¤‘', 'ì˜ì‚¬'],
        'ê°ì—¼ë³‘': ['ì½”ë¡œë‚˜', 'ë…ê°', 'ê°ì—¼ë³‘', 'ì „ì—¼', 'ê²©ë¦¬', 'ë°±ì‹ ', 'ì§ˆë³‘ê´€ë¦¬ì²­', 'í™•ì§„ì', 'ì§‘ë‹¨ê°ì—¼'],
        'ì‹¤ì¢…': ['ì‹¤ì¢…', 'í–‰ë°©ë¶ˆëª…', 'ì•„ì´ë¥¼ ì°¾ìŠµë‹ˆë‹¤', 'ì—°ë½ë‘ì ˆ', 'ì¹˜ë§¤ë…¸ì¸', 'ë¯¸ì•„', 'í–‰ë°©'],
        'í•´ì–‘ì‚¬ê³ ': ['ë°”ë‹¤', 'ì„ ë°•', 'ì¹¨ëª°', 'í‘œë¥˜', 'ì¡°ë‚œ', 'í•´ê²½', 'êµ¬ì¡°ìš”ì²­', 'ì–´ì„ ', 'í•´ìƒ', 'ì„ ì°½'],
        'ì‚°ì—…ì¬í•´': ['ê¸°ê³„', 'ì‚°ì—…í˜„ì¥', 'ì‘ì—… ì¤‘', 'ê³µì¥', 'ê¸°ê³„ì— ë¼ì„', 'ì ˆë‹¨ì‚¬ê³ ', 'ì‚°ì¬', 'í˜„ì¥ì‚¬ê³ '],
        'ê¸°íƒ€': ['ì†ŒìŒ', 'ì•…ì·¨', 'ë¶ˆí¸', 'ë¯¼ì›', 'ì •ì „', 'ë‹¨ìˆ˜', 'í™˜ê²½ì˜¤ì—¼']
    }

    detected_types: List[str] = []
    for cat, keywords in accident_types.items():
        if any(keyword in text for keyword in keywords):
            detected_types.append(cat)

    # ì‹œê°„ ì •ë³´
    time_patterns = [
        r'\d{1,2}ì‹œ\s*\d{1,2}ë¶„',
        r'\d{1,2}:\d{2}',
        r'(ì˜¤ëŠ˜|ì–´ì œ|ë‚´ì¼|ê¸ˆì¼|ë‹¹ì¼|ì£¼ë§)',
        r'\d{1,2}ì¼\s*ì „',
        r'(ë°©ê¸ˆ|ì¡°ê¸ˆ\s*ì „|ê³§)'
    ]
    time_info = []
    for pattern in time_patterns:
        time_info += re.findall(pattern, text)

    # ì¥ì†Œ ì •ë³´
    location_patterns = [
        r'[ê°€-í£]{2,10}ì‹œ\s*[ê°€-í£]{1,10}(êµ¬|êµ°)?\s*[ê°€-í£]{1,10}ë™',
        r'[ê°€-í£]+ë¡œ\s*\d+ê¸¸?',
        r'[ê°€-í£]+ë™\s*[ê°€-í£]+ì•„íŒŒíŠ¸',
        r'[ê°€-í£]+í•™êµ',
        r'[ê°€-í£]+ë³‘ì›',
        r'[ê°€-í£]+ì—­',
        r'[ê°€-í£]+í•­|[ê°€-í£]+í•´ë³€|[ê°€-í£]+ì•ë°”ë‹¤|[ê°€-í£]+í¬êµ¬'
    ]
    location_info = []
    for pattern in location_patterns:
        location_info += re.findall(pattern, text)

    # ê¸´ê¸‰ë„
    urgency_keywords = ['ê¸´ê¸‰', 'ì¦‰ì‹œ', 'ì‘ê¸‰', 'ìœ„í—˜', 'ì‚¬ë§', 'ì¤‘ìƒ', 'í­ë°œ', 'ì¡°ë‚œ', 'êµ¬ì¡°ìš”ì²­']
    is_urgent = any(word in text for word in urgency_keywords)

    # í”¼í•´ ì •ë³´
    damage_patterns = {
        'ì¸ëª…í”¼í•´': [
            r'\d+\s*ëª…\s*ì‚¬ë§', r'\d+\s*ëª…\s*ë¶€ìƒ',
            r'\d+\s*ëª…\s*ì¤‘ìƒ', r'\d+\s*ëª…\s*ê²½ìƒ'
        ],
        'ì¬ì‚°í”¼í•´': [
            r'\d+\s*(ë§Œ|ì–µ)\s*ì›\s*í”¼í•´',
            r'ì°¨ëŸ‰\s*íŒŒì†', r'ê±´ë¬¼\s*íŒŒì†', r'ì„¤ë¹„\s*ì†ìƒ'
        ],
        'ì°¨ëŸ‰ë²ˆí˜¸': [
            r'[0-9]{2,3}[ê°€-í£][0-9]{4}',
            r'[ê°€-í£]{2,3}\s*[0-9]{2,3}[ê°€-í£][0-9]{4}'
        ]
    }
    damage_info = {}
    for category, patterns in damage_patterns.items():
        damage_info[category] = []
        for pat in patterns:
            damage_info[category] += re.findall(pat, text)

    result = {
        "accident_types": detected_types,
        "time_info": time_info,
        "location_info": location_info,
        "is_urgent": is_urgent,
        "damage_info": damage_info
    }

    logger.info(f"âœ… ì œë³´ ê°ì§€ ì™„ë£Œ: {result}")
    return result


def create_report_summary_prompt(text: str, report_features: Dict) -> str:
    """ì œë³´ íŠ¹ì§•ì— ë§ëŠ” ìš”ì•½ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
    logger.info("ğŸ“ ì œë³´ ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±...")
    
    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
    base_prompt = "ë‹¤ìŒ ì œë³´ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”. "
    
    # ì‚¬ê³  ìœ í˜•ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸
    if 'êµí†µì‚¬ê³ ' in report_features['accident_types']:
        base_prompt += "êµí†µì‚¬ê³ ì˜ ê²½ìš° ì‹œê°„, ì¥ì†Œ, ì°¨ëŸ‰ì •ë³´, í”¼í•´ìƒí™©ì„ í¬í•¨í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”. "
    elif 'í™”ì¬' in report_features['accident_types']:
        base_prompt += "í™”ì¬ì˜ ê²½ìš° ë°œìƒì‹œê°„, ì¥ì†Œ, í”¼í•´ê·œëª¨, ì†Œë°©ëŒ€ ëŒ€ì‘ìƒí™©ì„ í¬í•¨í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”. "
    elif 'ë²”ì£„' in report_features['accident_types']:
        base_prompt += "ë²”ì£„ì˜ ê²½ìš° ë°œìƒì‹œê°„, ì¥ì†Œ, ë²”ì£„ìœ í˜•, í”¼í•´ìƒí™©ì„ í¬í•¨í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”. "
    
    # ê¸´ê¸‰ë„ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸
    if report_features['is_urgent']:
        base_prompt += "ê¸´ê¸‰í•œ ìƒí™©ì´ë¯€ë¡œ ì¦‰ì‹œ ëŒ€ì‘ì´ í•„ìš”í•œ ë‚´ìš©ì„ ê°•ì¡°í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”. "
    
    # ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
    if report_features['time_info']:
        base_prompt += f"ë°œìƒì‹œê°„({', '.join(report_features['time_info'])})ì„ í¬í•¨í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”. "
    
    # ì¥ì†Œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
    if report_features['location_info']:
        base_prompt += f"ë°œìƒì¥ì†Œ({', '.join(report_features['location_info'])})ë¥¼ í¬í•¨í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”. "
    
    # í”¼í•´ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
    if any(report_features['damage_info'].values()):
        damage_text = []
        for damage_type, info in report_features['damage_info'].items():
            if info:
                damage_text.append(f"{damage_type}: {', '.join(info)}")
        if damage_text:
            base_prompt += f"í”¼í•´ì •ë³´({'; '.join(damage_text)})ë¥¼ í¬í•¨í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”. "
    
    final_prompt = base_prompt + f"\n\nì œë³´ ë‚´ìš©: {text}"
    logger.info(f"âœ… ìƒì„±ëœ í”„ë¡¬í”„íŠ¸: {final_prompt[:200]}...")
    
    return final_prompt


def clean_summary(summary: str) -> str:
    """ìš”ì•½ ê²°ê³¼ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤"""
    logger.info("ğŸ§¹ ìš”ì•½ ì •ë¦¬ ì‹œì‘...")
    logger.info(f"ğŸ“ ì›ë³¸ ìš”ì•½: {summary}")
    
    # ëŒ€ê´„í˜¸ ì•ˆì˜ ë©”íƒ€ ì •ë³´ ì œê±°: [ì‹œê°„], [ìœ í˜•] ë“±
    summary = re.sub(r'\[[^\[\]]+\]', '', summary)
    
    # "ì‚¬ê±´ ê°œìš”:", "ìš”ì•½:" ë“±ì˜ í—¤ë” ì œê±°
    summary = re.sub(r'(ì‚¬ê±´\s*ê°œìš”\s*:?|ìš”ì•½\s*:?|ë‚´ìš©\s*:?)', '', summary)
    
    # í”„ë¡¬í”„íŠ¸ ê´€ë ¨ í…ìŠ¤íŠ¸ ì œê±°
    prompt_patterns = [
        r'ë‹¤ìŒ ì œë³´ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”\.?\s*',
        r'êµí†µì‚¬ê³ ì˜ ê²½ìš°.*?ìš”ì•½í•´ì£¼ì„¸ìš”\.?\s*',
        r'í™”ì¬ì˜ ê²½ìš°.*?ìš”ì•½í•´ì£¼ì„¸ìš”\.?\s*',
        r'ë²”ì£„ì˜ ê²½ìš°.*?ìš”ì•½í•´ì£¼ì„¸ìš”\.?\s*',
        r'ê¸´ê¸‰í•œ ìƒí™©ì´ë¯€ë¡œ.*?ìš”ì•½í•´ì£¼ì„¸ìš”\.?\s*',
        r'ë°œìƒì‹œê°„\([^)]*\)ì„ í¬í•¨í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”\.?\s*',
        r'ë°œìƒì¥ì†Œ\([^)]*\)ë¥¼ í¬í•¨í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”\.?\s*',
        r'í”¼í•´ì •ë³´\([^)]*\)ë¥¼ í¬í•¨í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”\.?\s*',
        r'ì œë³´ ë‚´ìš©:\s*',
        r'ìš”ì•½í•´ì£¼ì„¸ìš”\.?\s*',
        r'í¬í•¨í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”\.?\s*',
        r'ê°•ì¡°í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”\.?\s*',
        r'ë°œìƒì‹œê°„\([^)]*\)ì„ í¬í•¨í•˜ì—¬\.?\s*',
        r'ë°œìƒì¥ì†Œ\([^)]*\)ë¥¼ í¬í•¨í•˜ì—¬\.?\s*',
        r'í”¼í•´ì •ë³´\([^)]*\)ë¥¼ í¬í•¨í•˜ì—¬\.?\s*'
    ]
    
    for pattern in prompt_patterns:
        summary = re.sub(pattern, '', summary, flags=re.IGNORECASE | re.DOTALL)
    
    # ê³µë°± ì •ë¦¬
    lines = summary.strip().split('\n')
    cleaned_lines = [line.strip() for line in lines if line.strip()]
    
    # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë‹¤ì‹œ ì—°ê²°
    result = '\n'.join(cleaned_lines)
    
    # ìµœì¢… ê³µë°± ì •ë¦¬
    result = re.sub(r'\s+', ' ', result).strip()
    
    logger.info(f"âœ… ì •ë¦¬ëœ ìš”ì•½: {result}")
    return result


def extract_numbers(text: str) -> list:
    """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤"""
    numbers = re.findall(r'\d+', text)
    logger.info(f"ğŸ“Š ì¶”ì¶œëœ ìˆ«ì: {numbers}")
    return numbers


def find_sentences_with_numbers(text: str, numbers: list) -> list:
    """ìˆ«ìê°€ í¬í•¨ëœ ë¬¸ì¥ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤"""
    sentences = re.split(r'[.!?]\s*', text)
    sentences_with_numbers = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if any(num in sentence for num in numbers):
            sentences_with_numbers.append(sentence)
    
    logger.info(f"ğŸ”¢ ìˆ«ìê°€ í¬í•¨ëœ ë¬¸ì¥ ìˆ˜: {len(sentences_with_numbers)}")
    return sentences_with_numbers


def fix_incomplete_sentences(summary: str) -> str:
    """ë¶ˆì™„ì „í•œ ë¬¸ì¥ì„ ìˆ˜ì •í•©ë‹ˆë‹¤"""
    logger.info("ğŸ”§ ë¶ˆì™„ì „í•œ ë¬¸ì¥ ìˆ˜ì • ì‹œì‘...")
    
    sentences = summary.split('\n')
    fixed_sentences = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence:
            # ë¬¸ì¥ì´ ì™„ì „í•˜ì§€ ì•Šìœ¼ë©´ ìˆ˜ì •
            if not sentence.endswith(('.', '!', '?')):
                sentence += '.'
            fixed_sentences.append(sentence)
    
    result = '\n'.join(fixed_sentences)
    logger.info(f"âœ… ìˆ˜ì •ëœ ë¬¸ì¥: {result}")
    return result


def fix_korean_grammar(sentence: str) -> str:
    """í•œêµ­ì–´ ë¬¸ë²•ì„ ìˆ˜ì •í•©ë‹ˆë‹¤"""
    # ì¡°ì‚¬ ìˆ˜ì •
    sentence = re.sub(r'([ê°€-í£]+)ì—ì„œ\s+([ê°€-í£]+)ê°€', r'\1ì—ì„œ \2ì´', sentence)
    sentence = re.sub(r'([ê°€-í£]+)ì—ì„œ\s+([ê°€-í£]+)ë¥¼', r'\1ì—ì„œ \2ì„', sentence)
    
    # ì£¼ì–´-ëª©ì ì–´-ë™ì‚¬ ìˆœì„œ ê°œì„ 
    # "~ì—ì„œ ~ê°€ ~í–ˆë‹¤" â†’ "~ì—ì„œ ~ì´ ~í–ˆìŠµë‹ˆë‹¤"
    sentence = re.sub(r'([ê°€-í£]+)ì—ì„œ\s+([ê°€-í£]+)ê°€\s+([ê°€-í£]+)í–ˆë‹¤', r'\1ì—ì„œ \2ì´ \3í–ˆìŠµë‹ˆë‹¤', sentence)
    
    # ë¶ˆì™„ì „í•œ ë¬¸ì¥ êµ¬ì¡° ìˆ˜ì •
    if sentence.endswith('ë˜ëŠ”.'):
        sentence = sentence.replace('ë˜ëŠ”.', 'ë˜ëŠ” ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
    elif sentence.endswith('ë˜ëŠ”'):
        sentence = sentence.replace('ë˜ëŠ”', 'ë˜ëŠ” ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
    elif sentence.endswith('í•˜ëŠ”.'):
        sentence = sentence.replace('í•˜ëŠ”.', 'í•˜ëŠ” ìƒí™©ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
    elif sentence.endswith('í•˜ëŠ”'):
        sentence = sentence.replace('í•˜ëŠ”', 'í•˜ëŠ” ìƒí™©ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
    elif sentence.endswith('ìˆëŠ”.'):
        sentence = sentence.replace('ìˆëŠ”.', 'ìˆëŠ” ìƒíƒœì…ë‹ˆë‹¤.')
    elif sentence.endswith('ìˆëŠ”'):
        sentence = sentence.replace('ìˆëŠ”', 'ìˆëŠ” ìƒíƒœì…ë‹ˆë‹¤.')
    
    # êµ¬ì–´ì²´ë¥¼ ë¬¸ì–´ì²´ë¡œ ë³€ê²½
    sentence = re.sub(r'ëì–´ìš”', 'ë˜ì—ˆìŠµë‹ˆë‹¤', sentence)
    sentence = re.sub(r'ìˆì–´ìš”', 'ìˆìŠµë‹ˆë‹¤', sentence)
    sentence = re.sub(r'ì—†ì–´ìš”', 'ì—†ìŠµë‹ˆë‹¤', sentence)
    sentence = re.sub(r'í•´ìš”', 'í•©ë‹ˆë‹¤', sentence)
    sentence = re.sub(r'ë¼ìš”', 'ë©ë‹ˆë‹¤', sentence)
    sentence = re.sub(r'ë´ìš”', 'ë´…ë‹ˆë‹¤', sentence)
    sentence = re.sub(r'ì¤˜ìš”', 'ì¤ë‹ˆë‹¤', sentence)
    sentence = re.sub(r'ë‚˜ìš”', 'ë‚©ë‹ˆë‹¤', sentence)
    sentence = re.sub(r'ì–´ìš”', 'ìŠµë‹ˆë‹¤', sentence)
    sentence = re.sub(r'ì•„ìš”', 'ìŠµë‹ˆë‹¤', sentence)
    
    # ë¶ˆì™„ì „í•œ ë™ì‚¬ ìˆ˜ì •
    sentence = re.sub(r'(\w+)ë¼ì„œ', r'\1ë˜ì–´ì„œ', sentence)
    sentence = re.sub(r'(\w+)ëì–´ìš”', r'\1ë˜ì—ˆìŠµë‹ˆë‹¤', sentence)
    sentence = re.sub(r'(\w+)ëìŠµë‹ˆë‹¤', r'\1ë˜ì—ˆìŠµë‹ˆë‹¤', sentence)
    
    # ì¡°ì‚¬ ìˆ˜ì • (ì´/ê°€, ì„/ë¥¼)
    sentence = re.sub(r'([ê°€-í£]+)ê°€\s+([ê°€-í£]+)ë¥¼', r'\1ì´ \2ì„', sentence)
    sentence = re.sub(r'([ê°€-í£]+)ê°€\s+([ê°€-í£]+)ì—', r'\1ì´ \2ì—', sentence)
    
    return sentence


def improve_grammar_structure(summary: str) -> str:
    """ë¬¸ë²• êµ¬ì¡°ë¥¼ ê°œì„ í•©ë‹ˆë‹¤"""
    logger.info("ğŸ“ ë¬¸ë²• êµ¬ì¡° ê°œì„  ì‹œì‘...")
    logger.info(f"ğŸ“ ê°œì„  ì „: {summary}")
    
    sentences = summary.split('\n')
    improved_sentences = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence:
            # í•œêµ­ì–´ ë¬¸ë²• ìˆ˜ì •
            sentence = fix_korean_grammar(sentence)
            
            # ë¬¸ì¥ ë ì •ë¦¬
            if not sentence.endswith(('.', '!', '?')):
                sentence += '.'
            
            improved_sentences.append(sentence)
    
    result = '\n'.join(improved_sentences)
    logger.info(f"âœ… ê°œì„  í›„: {result}")
    return result


def improve_summary_tone(summary: str) -> str:
    """ìš”ì•½ì˜ ë§íˆ¬ë¥¼ ê°œì„ í•©ë‹ˆë‹¤"""
    logger.info("ğŸ¯ ë§íˆ¬ ê°œì„  ì‹œì‘...")
    logger.info(f"ğŸ“ ê°œì„  ì „: {summary}")
    
    # ë°˜ë³µë˜ëŠ” ë‚´ìš© ì œê±°
    summary = re.sub(r'ì œì£¼ì¹¼í˜¸í…”\s*', '', summary)
    summary = re.sub(r'ì œì£¼ ì„œê·€í¬ì¹¼í˜¸í…”\s*', 'ì œì£¼ ì„œê·€í¬ì¹¼í˜¸í…”', summary)
    
    # ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°
    summary = re.sub(r'ì¸ëª…í”¼í•´:\s*ì—†ìŒ\.?', '', summary)
    summary = re.sub(r'ì‚¬ê³ \s*ì¸ëª…í”¼í•´:\s*ì—†ìŒ\.?', '', summary)
    
    # ë§íˆ¬ ê°œì„ 
    summary = re.sub(r'ëì–´ìš”', 'ë˜ì—ˆìŠµë‹ˆë‹¤', summary)
    summary = re.sub(r'ìˆì–´ìš”', 'ìˆìŠµë‹ˆë‹¤', summary)
    summary = re.sub(r'ì—†ì–´ìš”', 'ì—†ìŠµë‹ˆë‹¤', summary)
    summary = re.sub(r'í•´ìš”', 'í•©ë‹ˆë‹¤', summary)
    summary = re.sub(r'ë¼ìš”', 'ë©ë‹ˆë‹¤', summary)
    
    # ë¬¸ì¥ ì •ë¦¬
    sentences = summary.split('\n')
    cleaned_sentences = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and len(sentence) > 5:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°
            # ì¤‘ë³µëœ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
            if not any(existing in sentence for existing in cleaned_sentences):
                cleaned_sentences.append(sentence)
    
    result = '\n'.join(cleaned_sentences)
    logger.info(f"âœ… ê°œì„  í›„: {result}")
    return result


def preserve_numbers_in_summary(original_text: str, summary: str) -> str:
    """ìš”ì•½ì—ì„œ ëˆ„ë½ëœ ì¤‘ìš”í•œ ìˆ«ìë“¤ì„ ì›ë³¸ì—ì„œ ë³µì›í•©ë‹ˆë‹¤"""
    logger.info("ğŸ”¢ ìˆ«ì ë³´ì¡´ ì²˜ë¦¬ ì‹œì‘...")
    
    original_numbers = extract_numbers(original_text)
    summary_numbers = extract_numbers(summary)
    
    # ìš”ì•½ì— ì—†ëŠ” ìˆ«ìë“¤ì„ ì°¾ì•„ì„œ ì¶”ê°€
    missing_numbers = [num for num in original_numbers if num not in summary_numbers]
    logger.info(f"ğŸ“Š ëˆ„ë½ëœ ìˆ«ì: {missing_numbers}")
    
    if missing_numbers:
        # ëˆ„ë½ëœ ìˆ«ìê°€ í¬í•¨ëœ ë¬¸ì¥ë“¤ì„ ì°¾ê¸°
        sentences_with_missing_numbers = find_sentences_with_numbers(original_text, missing_numbers)
        
        # ìš”ì•½ì— ì¶”ê°€í•  ë¬¸ì¥ë“¤
        additional_sentences = []
        for sentence in sentences_with_missing_numbers:
            if sentence not in summary and len(sentence) > 10:  # ì¤‘ë³µ ì œê±° ë° ìµœì†Œ ê¸¸ì´
                additional_sentences.append(sentence)
        
        # ì¶”ê°€ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ìš”ì•½ì— ì¶”ê°€
        if additional_sentences:
            summary += '\n' + '\n'.join(additional_sentences[:2])  # ìµœëŒ€ 2ê°œ ë¬¸ì¥ë§Œ ì¶”ê°€
            logger.info(f"â• ì¶”ê°€ëœ ë¬¸ì¥: {additional_sentences[:2]}")
    
    logger.info(f"âœ… ìµœì¢… ìš”ì•½: {summary}")
    return summary


def summarize_text(text: str) -> str:
    """T5 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤"""
    global model, tokenizer
    
    logger.info("ğŸš€ ìš”ì•½ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
    logger.info(f"ğŸ“„ ì…ë ¥ í…ìŠ¤íŠ¸: {text[:100]}...")
    
    try:
        # ì œë³´ íŠ¹ì§• ê°ì§€
        logger.info("ğŸ” ì œë³´ íŠ¹ì§• ê°ì§€ ì‹œì‘...")
        report_features = detect_report_type(text)
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        logger.info("ğŸ§¹ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œì‘...")
        cleaned_text = clean_news(text)
        logger.info(f"âœ… ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸: {cleaned_text[:100]}...")
        
        if not cleaned_text.strip():
            logger.warning("âš ï¸ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            return text
        
        # ì œë³´ íŠ¹ì§•ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
        logger.info("ğŸ“ ì œë³´ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±...")
        input_text = create_report_summary_prompt(cleaned_text, report_features)
        logger.info(f"ğŸ”¤ T5 ì…ë ¥: {input_text[:200]}...")
        
        # í† í¬ë‚˜ì´ì§•
        logger.info("ğŸ”¤ í† í¬ë‚˜ì´ì§• ì‹œì‘...")
        input_ids = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512).input_ids
        logger.info(f"âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ (ê¸¸ì´: {input_ids.shape[1]})")
        
        # ì œë³´ ìœ í˜•ì— ë”°ë¥¸ ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •
        generation_params = get_generation_params_by_type(report_features)
        
        # ìš”ì•½ ìƒì„±
        logger.info("ğŸ¤– T5 ëª¨ë¸ ìš”ì•½ ìƒì„± ì‹œì‘...")
        output_ids = model.generate(
            input_ids, 
            **generation_params
        )
        logger.info(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ (ì¶œë ¥ ê¸¸ì´: {output_ids.shape[1]})")
        
        # ë””ì½”ë”©
        logger.info("ğŸ”¤ ë””ì½”ë”© ì‹œì‘...")
        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        logger.info(f"âœ… ì›ë³¸ ìš”ì•½: {summary}")
        
        # í”„ë¡¬í”„íŠ¸ ì œê±° (T5ê°€ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•œ ê²½ìš°)
        summary = clean_summary(summary)
        
        # ì œë³´ íŠ¹í™” í›„ì²˜ë¦¬
        summary = post_process_report_summary(summary, report_features, text)
        
        logger.info("ğŸ‰ ìš”ì•½ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        return summary
        
    except Exception as e:
        logger.error(f"âŒ T5 ìš”ì•½ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
        return text


def get_generation_params_by_type(report_features: Dict) -> Dict:
    """ì œë³´ ìœ í˜•ì— ë”°ë¥¸ ìƒì„± íŒŒë¼ë¯¸í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"""
    base_params = {
        'max_length': 100,
        'min_length': 20,
        'num_beams': 4,
        'early_stopping': True,
        'do_sample': True,
        'no_repeat_ngram_size': 3,
        'repetition_penalty': 1.1,
        'length_penalty': 0.8,
        'temperature': 0.8,
        'top_k': 50,
        'top_p': 0.9
    }
    
    # ê¸´ê¸‰í•œ ê²½ìš° ë” ìƒì„¸í•œ ìš”ì•½
    if report_features['is_urgent']:
        base_params['max_length'] = 150
        base_params['min_length'] = 30
        base_params['temperature'] = 0.7  # ë” ì¼ê´€ëœ ì¶œë ¥
    
    # êµí†µì‚¬ê³ ì˜ ê²½ìš° ì°¨ëŸ‰ì •ë³´ í¬í•¨
    if 'êµí†µì‚¬ê³ ' in report_features['accident_types']:
        base_params['max_length'] = 120
        base_params['min_length'] = 25
    
    # í™”ì¬ì˜ ê²½ìš° ë” ìƒì„¸í•œ ì •ë³´
    if 'í™”ì¬' in report_features['accident_types']:
        base_params['max_length'] = 140
        base_params['min_length'] = 30
    
    return base_params


def post_process_report_summary(summary: str, report_features: Dict, original_text: str) -> str:
    """ì œë³´ ìš”ì•½ì„ í›„ì²˜ë¦¬í•©ë‹ˆë‹¤"""
    logger.info("ğŸ”§ ì œë³´ íŠ¹í™” í›„ì²˜ë¦¬ ì‹œì‘...")
    
    # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ìˆ˜ì •
    summary = fix_incomplete_sentences(summary)
    
    # ë¬¸ë²• êµ¬ì¡° ê°œì„ 
    summary = improve_grammar_structure(summary)
    
    # ë§íˆ¬ ê°œì„ 
    summary = improve_summary_tone(summary)
    
    # ìˆ«ì ë³´ì¡´
    summary = preserve_numbers_in_summary(original_text, summary)
    
    # ì œë³´ íŠ¹í™” ì •ë¦¬
    summary = format_report_summary(summary, report_features)
    
    logger.info(f"âœ… í›„ì²˜ë¦¬ ì™„ë£Œ: {summary}")
    return summary


def format_report_summary(summary: str, report_features: Dict) -> str:
    """ì œë³´ ìš”ì•½ì„ í˜•ì‹ì— ë§ê²Œ ì •ë¦¬í•©ë‹ˆë‹¤"""
    logger.info("ğŸ“‹ ì œë³´ í˜•ì‹ ì •ë¦¬ ì‹œì‘...")
    
    # ê¸´ê¸‰í•œ ê²½ìš° í‘œì‹œ
    if report_features['is_urgent']:
        summary = "[ê¸´ê¸‰] " + summary
    
    # ì‚¬ê³  ìœ í˜• í‘œì‹œ
    if report_features['accident_types']:
        accident_type = report_features['accident_types'][0]
        summary = f"[{accident_type}] " + summary
    
    # ì‹œê°„ ì •ë³´ê°€ ìˆìœ¼ë©´ ì•ì— ë°°ì¹˜
    if report_features['time_info']:
        time_info = report_features['time_info'][0]
        if not summary.startswith(f"[{time_info}]"):
            summary = f"[{time_info}] " + summary
    
    # ì¥ì†Œ ì •ë³´ê°€ ìˆìœ¼ë©´ í¬í•¨
    if report_features['location_info'] and not any(loc in summary for loc in report_features['location_info']):
        location_info = report_features['location_info'][0]
        summary = summary.replace(".", f" ({location_info}).", 1)
    
    logger.info(f"âœ… í˜•ì‹ ì •ë¦¬ ì™„ë£Œ: {summary}")
    return summary 